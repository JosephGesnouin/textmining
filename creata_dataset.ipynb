{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = stopwords.words('english')\n",
    "common_words = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - GENE\n",
    "\n",
    "2 - DISEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../genes.txt\") as g:\n",
    "    genes = set([ x.lower().strip() for x in g.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.append(\"ace\")\n",
    "st.append(\"large\")\n",
    "st.append(\"kit\")\n",
    "st.append(\"impact\")\n",
    "st.append(\"set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 16580/16580 [00:37<00:00, 437.84it/s]\n"
     ]
    }
   ],
   "source": [
    "genes_clean = []\n",
    "for g in tqdm(genes):\n",
    "    if g not in st and not g.isdigit() and g not in common_words:\n",
    "        genes_clean.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16278"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genes_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16278"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genes_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Maladies.txt\") as m:\n",
    "    maladies = list(set([ x.lower().strip() for x in m.readlines()]))\n",
    "    maladies.remove(\"disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = open(\"../asthma.json\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 44975/44975 [41:47<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_DATA_WORD=[]\n",
    "train_i = 0\n",
    "nb = 0\n",
    "for line in tqdm(datas):\n",
    "    text = json.loads(line)[\"ab\"]\n",
    "    train_i += 1\n",
    "    \n",
    "    detected_disease = []\n",
    "    for maladie in maladies:\n",
    "        if maladie in text:\n",
    "            detected_disease.append(maladie.lower())\n",
    "    \n",
    "    for sentence in tokenize.sent_tokenize(text):\n",
    "        if len(sentence) > 10:\n",
    "            uhm = {\"entities\": []}\n",
    "            \n",
    "            for detect in detected_disease:\n",
    "                if detect in sentence.lower():\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(detect), sentence.lower())]\n",
    "                    for qs in pmz:\n",
    "                        uhm[\"entities\"].append( (qs, qs+len(detect), \"DISEASE\")  )\n",
    "            \n",
    "            for n in tokenize.word_tokenize(sentence):\n",
    "                if n.lower() in genes_clean:\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n.lower()), sentence.lower())]\n",
    "                    for qs in pmz:\n",
    "                        uhm[\"entities\"].append( (qs, qs+len(n), \"GENE\")  )\n",
    "            \n",
    "            TRAIN_DATA_WORD.append( (sentence, uhm) )\n",
    "    \n",
    "    if train_i % 2000 == 0:\n",
    "        with open(\"train/normal/train_set_\"+str(nb)+\".json\", \"w\") as t:\n",
    "            t.write(json.dumps(TRAIN_DATA_WORD))\n",
    "        nb += 1\n",
    "        TRAIN_DATA_WORD=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "alld = []\n",
    "allg = []\n",
    "for dg in TRAIN_DATA_WORD[0:1000]:\n",
    "    if dg[1][\"entities\"] :\n",
    "        for d  in dg[1][\"entities\"]:\n",
    "            if d[2] == \"DISEASE\":\n",
    "                allg.append(dg[0][d[0]:d[1]])\n",
    "            if d[2] == \"GENE\":\n",
    "#                 print()\n",
    "#                 print(dg[0][d[0]:d[1]], d[2])\n",
    "#                 if dg[0][d[0]:d[1]] == \"set\":\n",
    "#                     print(dg)\n",
    "                alld.append(dg[0][d[0]:d[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Asthma',\n",
       " 'Atopic dermatitis',\n",
       " 'Fibrous dysplasia',\n",
       " 'Food allergy',\n",
       " 'Herpes zoster',\n",
       " 'Influenza',\n",
       " 'Leukemia',\n",
       " 'Obesity',\n",
       " 'Osteoporosis',\n",
       " 'allergic asthma',\n",
       " 'allergic rhinitis',\n",
       " 'asthma',\n",
       " 'atopic dermatitis',\n",
       " 'bronchiolitis',\n",
       " 'cancer',\n",
       " 'carcinoma',\n",
       " 'chronic obstructive pulmonary disease',\n",
       " 'coronary heart disease',\n",
       " 'dermatitis',\n",
       " 'diabetes mellitus',\n",
       " 'egg allergy',\n",
       " 'fibrous dysplasia',\n",
       " 'food allergy',\n",
       " 'gestational diabetes',\n",
       " 'heart disease',\n",
       " 'herpes zoster',\n",
       " 'hypertension',\n",
       " 'hypoglycemia',\n",
       " 'influenza',\n",
       " 'irritable bowel syndrome',\n",
       " 'kidney disease',\n",
       " 'leukemia',\n",
       " 'lung cancer',\n",
       " 'lung carcinoma',\n",
       " 'lung disease',\n",
       " 'lymphoma',\n",
       " 'myasthenia gravis',\n",
       " 'myocardial infarction',\n",
       " 'myocarditis',\n",
       " 'obesity',\n",
       " 'obstructive lung disease',\n",
       " 'optic atrophy',\n",
       " 'osteoarthritis',\n",
       " 'osteoporosis',\n",
       " 'ovarian cancer',\n",
       " 'pneumonia',\n",
       " 'propionic acidemia',\n",
       " 'psoriasis',\n",
       " 'pulmonary fibrosis',\n",
       " 'rhinitis',\n",
       " 'sarcoidosis',\n",
       " 'syndrome',\n",
       " 'urticaria'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(allg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APP',\n",
       " 'BCL2',\n",
       " 'CCL21',\n",
       " 'CD4',\n",
       " 'CHIT1',\n",
       " 'CLPs',\n",
       " 'Cs',\n",
       " 'EED',\n",
       " 'ERBB4',\n",
       " 'FEV',\n",
       " 'HR',\n",
       " 'HTA',\n",
       " 'IGF1R',\n",
       " 'IQGAP2',\n",
       " 'MyD88',\n",
       " 'NHS',\n",
       " 'NLRP3',\n",
       " 'NMs',\n",
       " 'NPs',\n",
       " 'Ndst1',\n",
       " 'OXR1',\n",
       " 'RNASET2',\n",
       " 'RhoA',\n",
       " 'Sptlc2',\n",
       " 'TGM3',\n",
       " 'TLR2',\n",
       " 'TLR7',\n",
       " 'TLR8',\n",
       " 'TNF',\n",
       " 'TRAF6',\n",
       " 'TSLP'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(alld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# TRAIN_DATA_WORD=[]\n",
    "# train_i = 0\n",
    "# nb = 0\n",
    "# for line in tqdm(datas[0:2]):\n",
    "#     text = json.loads(line)[\"ab\"].lower()\n",
    "#     train_i += 1\n",
    "    \n",
    "#     detected_disease = []\n",
    "#     for maladie in maladies:\n",
    "#         if maladie in text:\n",
    "#             detected_disease.append(k)\n",
    "    \n",
    "#     for sentence in tokenize.sent_tokenize(text):\n",
    "#         if len(sentence) > 2:\n",
    "#             uhm = {\"entities\": []}\n",
    "            \n",
    "#             for detect in detected_disease:\n",
    "#                 if detect in sentence:\n",
    "#                     pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n2), sentence)]\n",
    "#                     for qs in pmz:\n",
    "#                         uhm[\"entities\"].append( (qs, qs+len(n2), \"DISEASE\")  )\n",
    "            \n",
    "            \n",
    "#             for n in sentence.split(\" \"):\n",
    "#                 n2 = n.replace(\".\", \"\").replace(\",\", \"\")\n",
    "#                 if n2 in maladies:\n",
    "#                     pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n2), sentence)]\n",
    "#                     for qs in pmz:\n",
    "#                         uhm[\"entities\"].append( (qs, qs+len(n2), \"DISEASE\")  )\n",
    "#                 if n2 in genes_clean:\n",
    "#                     pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n2), sentence)]\n",
    "#                     for qs in pmz:\n",
    "#                         uhm[\"entities\"].append( (qs, qs+len(n2), \"GENE\")  )\n",
    "            \n",
    "            \n",
    "            \n",
    "#             TRAIN_DATA_WORD.append( (sentence, uhm) )\n",
    "    \n",
    "# #     if train_i % 2000 == 0:\n",
    "# #         with open(\"train/train_set_\"+str(nb)+\".json\", \"w\") as t:\n",
    "# #             t.write(json.dumps(TRAIN_DATA_WORD))\n",
    "# #         nb += 1\n",
    "# #         TRAIN_DATA_WORD=[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
