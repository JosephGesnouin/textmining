{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = stopwords.words('english')\n",
    "common_words = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - GENE\n",
    "\n",
    "2 - DISEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:cours/textmining/genes.txt\") as g:\n",
    "    genes = set([ x.lower().strip() for x in g.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.append(\"ace\")\n",
    "st.append(\"large\")\n",
    "st.append(\"kit\")\n",
    "st.append(\"impact\")\n",
    "st.append(\"set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16580/16580 [00:39<00:00, 419.05it/s]\n"
     ]
    }
   ],
   "source": [
    "genes_clean = []\n",
    "for g in tqdm(genes):\n",
    "    if g not in st and not g.isdigit() and g not in common_words:\n",
    "        genes_clean.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16278"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genes_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16415"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genes_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:cours/textmining/Maladies.txt\") as m:\n",
    "    maladies = list(set([ x.lower().strip() for x in m.readlines()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = open(\"D:cours/textmining/asthma.json\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44975/44975 [41:22<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_DATA_WORD=[]\n",
    "train_i = 0\n",
    "nb = 0\n",
    "for line in tqdm(datas):\n",
    "    text = json.loads(line)[\"ab\"].lower()\n",
    "    train_i += 1\n",
    "    \n",
    "    detected_disease = []\n",
    "    for maladie in maladies:\n",
    "        if maladie in text:\n",
    "            detected_disease.append(maladie)\n",
    "    \n",
    "    for sentence in tokenize.sent_tokenize(text):\n",
    "        if len(sentence) > 2:\n",
    "            uhm = {\"entities\": []}\n",
    "            \n",
    "            for detect in detected_disease:\n",
    "                if detect in sentence:\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(detect), sentence)]\n",
    "                    for qs in pmz:\n",
    "                        uhm[\"entities\"].append( (qs, qs+len(detect), \"DISEASE\")  )\n",
    "            \n",
    "            for n in tokenize.word_tokenize(sentence):\n",
    "                if n in genes_clean:\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n), sentence)]\n",
    "                    for qs in pmz:\n",
    "                        uhm[\"entities\"].append( (qs, qs+len(n), \"GENE\")  )\n",
    "            \n",
    "            TRAIN_DATA_WORD.append( (sentence, uhm) )\n",
    "    \n",
    "    if train_i % 2000 == 0:\n",
    "        with open(\"train/train_set_\"+str(nb)+\".json\", \"w\") as t:\n",
    "            t.write(json.dumps(TRAIN_DATA_WORD))\n",
    "        nb += 1\n",
    "        TRAIN_DATA_WORD=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alld = []\n",
    "allg = []\n",
    "for dg in TRAIN_DATA_WORD[0:1000]:\n",
    "    if dg[1][\"entities\"] :\n",
    "        for d  in dg[1][\"entities\"]:\n",
    "            if d[2] == \"DISEASE\":\n",
    "                allg.append(dg[0][d[0]:d[1]])\n",
    "            if d[2] == \"GENE\":\n",
    "#                 print()\n",
    "#                 print(dg[0][d[0]:d[1]], d[2])\n",
    "#                 if dg[0][d[0]:d[1]] == \"set\":\n",
    "#                     print(dg)\n",
    "                alld.append(dg[0][d[0]:d[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app',\n",
       " 'bcl2',\n",
       " 'ccl21',\n",
       " 'cd4',\n",
       " 'chit1',\n",
       " 'clps',\n",
       " 'cs',\n",
       " 'eed',\n",
       " 'erbb4',\n",
       " 'fev',\n",
       " 'hr',\n",
       " 'hta',\n",
       " 'igf1r',\n",
       " 'iqgap2',\n",
       " 'myd88',\n",
       " 'ndst1',\n",
       " 'nhs',\n",
       " 'nlrp3',\n",
       " 'nms',\n",
       " 'nps',\n",
       " 'oxr1',\n",
       " 'rhoa',\n",
       " 'rnaset2',\n",
       " 'sptlc2',\n",
       " 'tgm3',\n",
       " 'tlr2',\n",
       " 'tlr7',\n",
       " 'tlr8',\n",
       " 'tnf',\n",
       " 'traf6',\n",
       " 'tslp'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(alld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# TRAIN_DATA_WORD=[]\n",
    "# train_i = 0\n",
    "# nb = 0\n",
    "# for line in tqdm(datas[0:2]):\n",
    "#     text = json.loads(line)[\"ab\"].lower()\n",
    "#     train_i += 1\n",
    "    \n",
    "#     detected_disease = []\n",
    "#     for maladie in maladies:\n",
    "#         if maladie in text:\n",
    "#             detected_disease.append(k)\n",
    "    \n",
    "#     for sentence in tokenize.sent_tokenize(text):\n",
    "#         if len(sentence) > 2:\n",
    "#             uhm = {\"entities\": []}\n",
    "            \n",
    "#             for detect in detected_disease:\n",
    "#                 if detect in sentence:\n",
    "#                     pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n2), sentence)]\n",
    "#                     for qs in pmz:\n",
    "#                         uhm[\"entities\"].append( (qs, qs+len(n2), \"DISEASE\")  )\n",
    "            \n",
    "            \n",
    "#             for n in sentence.split(\" \"):\n",
    "#                 n2 = n.replace(\".\", \"\").replace(\",\", \"\")\n",
    "#                 if n2 in maladies:\n",
    "#                     pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n2), sentence)]\n",
    "#                     for qs in pmz:\n",
    "#                         uhm[\"entities\"].append( (qs, qs+len(n2), \"DISEASE\")  )\n",
    "#                 if n2 in genes_clean:\n",
    "#                     pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n2), sentence)]\n",
    "#                     for qs in pmz:\n",
    "#                         uhm[\"entities\"].append( (qs, qs+len(n2), \"GENE\")  )\n",
    "            \n",
    "            \n",
    "            \n",
    "#             TRAIN_DATA_WORD.append( (sentence, uhm) )\n",
    "    \n",
    "# #     if train_i % 2000 == 0:\n",
    "# #         with open(\"train/train_set_\"+str(nb)+\".json\", \"w\") as t:\n",
    "# #             t.write(json.dumps(TRAIN_DATA_WORD))\n",
    "# #         nb += 1\n",
    "# #         TRAIN_DATA_WORD=[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
